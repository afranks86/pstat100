---
title: "PSTAT 100 Lab 5"
format: pdf
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message =  FALSE)
knitr::opts_chunk$set(warning =  FALSE)
knitr::opts_chunk$set(error =  FALSE)
bfcolor <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{\\textbf{%s}}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'><b>%s</b></span>", color, x)
  } else x
}

# Install necessary libraries if they aren't installed
if (!require(dplyr)) install.packages("dplyr")
if (!require(tidyr)) install.packages("tidyr")
if (!require(ggplot2)) install.packages("ggplot2")
```

# Lab 5: Principle Components

Principal components analysis (PCA) is a widely-used multivariate analysis technique. Depending on the application, PCA is variously described as:

-   a dimension reduction method
-   a an approximation method
-   a latent factor model
-   a filtering or compression method

The core technique of PCA is *finding linear data transformations that preserve variance*.

What does it mean to say that '*principal components are linear data transformations*'? Suppose you have a dataset with $n$ observations and $p$ variables. We can represent the values as a data matrix $\mathbf{X}$ with $n$ rows and $p$ columns:

$$ \mathbf{X}  = \underbrace{\left[\begin{array}{cccc}     \mathbf{x}_1 &\mathbf{x}_2 &\cdots &\mathbf{x}_p     \end{array}\right]}_{\text{column vectors}} = \left[\begin{array}{cccc}     x_{11} &x_{12} &\cdots &x_{1p} \\     x_{21} &x_{22} &\cdots &x_{2p} \\     \vdots &\vdots &\ddots &\vdots \\     x_{n1} &x_{n2} &\cdots &x_{np} \end{array}\right] $$

To say that the principal components are linear data transformations means that each principal component is of the form:

$$ \text{PC} = \mathbf{Xv} = v_1 \mathbf{x}_1 + v_2 \mathbf{x}_2 + \cdots + v_p \mathbf{x}_p $$

for some vector $\mathbf{v}$. In PCA, the following terminology is used:

-   linear combination coefficients $v_j$ are known as *loadings*
-   values of the linear combinations are known as *scores*
-   the vector of loadings $\mathbf{v}$ is known as a *principal axis*

As discussed in lecture, the values of the loadings are found by decomposing the correlation structure.

```{r}
# Load libraries
library(dplyr)
library(ggplot2)
```

## Objectives

In this lab, you'll focus on computing and interpreting principal components:

-   finding the loadings (linear combination coefficients) for each PC;
-   quantifying the variation captured by each PC;
-   visualization-based techniques for selecting a number of PC's to analyze;
-   plotting and interpreting loadings.

You'll work with a selection of county summaries from the 2010 U.S. census. The first few rows of the dataset are shown below:

```{r}
# Import tidy county-level 2010 census data
census <- read.csv('data/census2010.csv', fileEncoding = 'latin1')
head(census)
```

The observational units are U.S. counties, and each row is an observation on one county. The values are, for the most part, percentages of the county population. You can find variable descriptions in the metadata file `census2010metadata.csv` in the data directory.

### Correlations

PCA identifies variable combinations that capture covariation by decomposing the correlation matrix. So, to start with, let's examine the correlation matrix for the 2010 county-level census data to get a sense of which variables tend to vary together.

The correlation matrix is a matrix of all pairwise correlations between variables. If $x_ij$ denotes the value for the $i^{\text{th}}$ observation of variable $j$, then the entry at row $j$ and column $k$ of the correlation matrix $\mathbf{R}$ is:

$$r_{jk} = \frac{\sum_i (x_{ij} - \bar{x}_j)(x_{ik} - \bar{x}_k)}{S_j S_k}$$

In the census data, the `State` and `County` columns indicate the geographic region for each observation; essentially, they are a row index. So we'll drop them before computing the matrix $\mathbf{R}$:

```{r}
# Drop 'State' and 'County' columns
x_mx <- census[, !(names(census) %in% c('State', 'County'))]

```

From here, the matrix is simple to compute using `cor()`

```{r}
# Compute the correlation matrix
corr_mx <- cor(x_mx, use = "pairwise.complete.obs")
```

The matrix can be inspected directly to determine which variables vary together. For example, we could look at the correlations between employment rate and every other variable in the dataset by extracting the `Employed` column from the correlation matrix and sorting the correlations:

```{r}
# Correlation between employment rate and other variables, sorted
sorted_corr <- sort(corr_mx[, 'Employed'])
sorted_corr
```

Recall that correlation is a number in the interval \[-1, 1\] whose magnitude indicates the strength of the linear relationship between variables:

-   correlations near -1 are *strongly negative*, and mean that the variables *tend to vary in opposition*
-   correlations near 1 are *strongly positive*, and mean that the variables *tend to vary together*

From examining the output above, it can be seen that the percentage of the county population that is employed is:

-   strongly *negatively* correlated with child poverty, poverty, and unemployment, meaning it *tends to vary in opposition* with these variables
-   strongly *positively* correlated with income per capita, meaning it *tends to vary together* with this variable

If instead we wanted to look up the correlation between just two variables, we could retrieve the relevant entry directly using `corr_mx['...','...']` with the variable names:

```{r}
# Correlation between employment and income per capita
corr_mx['Employed', 'IncomePerCap']

```

So across U.S. counties employment is, perhaps unsurprisingly, strongly and positively correlated with income per capita, meaning that higher employment rates tend to coincide with higher incomes per capita.

## Question 1

Find the correlation between the poverty rate and demographic minority rate and store the value as `pov_dem_rate`. Interpret the value in context.

`r bfcolor("YOUR ANSWER:", "red")`

*(Type your answer here, replacing this text.)*

```{r}
# correlation between poverty and percent minority
# pov_dem_rate <- ...

# print
# pov_dem_rate
```

While direct inspection is useful, it can be cumbersome to check correlations for a large number of variables this way. A heatmap -- a colored image of the matrix -- provides a (sometimes) convenient way to see what's going on without having to examine the numerical values directly. The cell below shows one way of constructing this plot. Notice the diverging color scale; this should always be used.

```{r}
# Melt (reshape) the correlation matrix into long format
corr_mx_long <- as.data.frame(corr_mx) %>%
  tibble::rownames_to_column(var = "row") %>%
  pivot_longer(cols = -row, names_to = "col", values_to = "Correlation")

# Construct heatmap using ggplot2
ggplot(corr_mx_long, aes(x = col, y = row, fill = Correlation)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "orange", midpoint = 0, 
                       limits = c(-1, 1)) +
  labs(x = "", y = "", fill = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_fixed() # Ensures square aspect ratio

```

## Question 2

Which variable is self employment rate most *positively* correlated with? Refer to the heatmap.

`r bfcolor("YOUR ANSWER:", "red")`

(*Type your answer here, replacing this text.*)

### Computing principal components

Each principal component is of the form:

$$\text{PC}_{i} = \sum_j v_j x_{ij} \quad(\text{PC score for observation } i)$$

The loading $v_j$ for each component indicate which variables are most influential (heavily weighted) on that principal axis, and thus offer an indirect picture of which variables are driving variation and covariation in the original data.

#### Loadings and scores

In **R**, the `prcomp()` function from the **stats** package provides an easy-to-use implementation for Principal Component Analysis (PCA). Alternatively, the **FactoMineR** or **psych** packages offer more detailed PCA methods.

```{r}
# Perform PCA with standardization
pca <- prcomp(x_mx, center = TRUE, scale. = TRUE)
```

In **R**, most quantities you might need for PCA can be accessed as attributes of the `pca` object. Specifically:

-   `pca$rotation` contains the **loadings** (principal component coefficients).

-   `pca$x` contains the **scores** (principal component projections of the data).

-   `pca$sdev^2` contains the **eigenvalues** (variances along each principal axis, see lecture notes).

Examine the loadings below. Each column represents the loadings for one principal component, with components ordered from largest to smallest variance

::: callout
```{r}
# Inspect loadings
pca$rotation
```
:::

Similarly, inspect the scores below and check your understanding; each row is an observation and the columns give the scores on each principal axis.

```{r}
# Compute variance of PCA scores
apply(pca$x, 2, var)
```

Importantly, `statsmodels` rescales the scores so that they have unit inner product; in other words, so that the variances are all $\frac{1}{n - 1}$.

```{r}
# Variance of PCA scores
scores_var <- apply(pca$x, 2, var)
scores_var
```

```{r}
# For comparison
1 / (nrow(x_mx) - 1)
```

In **R**, to change this behavior and disable normalization (i.e., prevent standardization of variables), set `scale. = FALSE` when computing the principal components using `prcomp()`

## Question 3

Check your understanding. Which variable contributes most to the sixth principal component? Store the variable name exactly as it appears among the original column names as `pc6_most_influential_variable`, and store the corresponding loading as `pc6_most_influential_variable_loading`. Print the variable name.

`r bfcolor("YOUR ANSWER:", "red")`\

```{r}
# find most influential variable
# pc6_most_influential_variable <- ...

# find loading
# pc6_most_influential_variable_loading <- ...

# print
```

### Variance ratios

The *variance ratios* indicate the proportions of total variance in the data captured by each principal axis. You may recall from lecture that the variance ratios are computed from the eigenvalues of the correlation (or covariance, if data are not standardized) matrix.

When using `statsmodels`, these need to be computed manually.

```{r}
# Compute variance ratios
var_ratios <- (pca$sdev^2) / sum(pca$sdev^2)

# Print variance ratios
print(var_ratios)
```

Note again that the principal components have been computed in order of *decreasing* variance.

## Question 4

Check your understanding. What proportion of variance is captured *jointly* by the first three components taken together? Provide a calculation to justify your answer.

`r bfcolor("YOUR ANSWER:", "red")`

(*Type your answer here, replacing this text.)*

#### Selecting a subset of PCs

PCA generally consists of choosing a small subset of components. The basic strategy for selecting this subset is to determine how many are needed to capture some analyst-chosen minimum portion of total variance in the original data.

Most often this assessment is made graphically by inspecting the variance ratios and their cumulative sum, *i.e.*, the amount of total variation captured jointly by subsets of successive components. We'll store these quantities in a data frame.

::: callout
```{r}
# Load necessary library
library(dplyr)

# Create the data frame first
pca_var_explained <- data.frame(
  Component = seq(1, length(var_ratios)),
  Proportion_of_variance_explained = var_ratios
)

# Add cumulative variance explained
pca_var_explained <- pca_var_explained %>%
  mutate(Cumulative_variance_explained = cumsum(Proportion_of_variance_explained))

# Print first few rows
head(pca_var_explained)
```
:::

Now we'll make a dual-axis plot showing, on one side, the proportion of variance explained (y) as a function of component (x), and on the other side, the cumulative variance explained (y) also as a function of component (x). Make sure that you've completed Q1(a) before running the next cell.

```{r}
# Create the base plot
var_explained_plot <- ggplot(pca_var_explained, aes(x = Component)) +
  
  # Proportion of variance explained (green line & points)
  geom_line(aes(y = Proportion_of_variance_explained, color = "Proportion of variance explained"), size = 1) +
  geom_point(aes(y = Proportion_of_variance_explained, color = "Proportion of variance explained"), size = 2) +
  
  # Cumulative variance explained (blue line & points)
  geom_line(aes(y = Cumulative_variance_explained, color = "Cumulative variance explained"), size = 1) +
  geom_point(aes(y = Cumulative_variance_explained, color = "Cumulative variance explained"), size = 2) +
  
  # Custom colors for lines
  scale_color_manual(values = c("Proportion of variance explained" = "#57A44C", 
                                "Cumulative variance explained" = "#5276A7")) +
  
  # Axis labels and theme adjustments
  labs(x = "Component", y = "Variance Explained", color = "Legend") +
  theme_minimal()

# Display the plot
print(var_explained_plot)
```

The purpose of making this plot is to quickly determine the fewest number of principal components that capture a considerable portion of variation and covariation. 'Considerable' here is a bit subjective.

## Question 5

How many principal components explain more than 6% of total variation individually? Store this number as `num_pc`, and store the proportion of variation that they capture jointly as `var_explained`.

`r bfcolor("YOUR ANSWER:", "red")`\

```{r}
# number of selected components
# num_pc <- ...

# variance explained
# var_explained <- ...

#print
# print('number selected: ', num_pc)
# print('proportion of variance captured: ', var_explained)
```

### Interpreting loadings

Now that you've chosen the number of components to work with, the next step is to examine loadings to understand just *which* variables the components combine with significant weight.

We'll store the scores for the components you selected as a dataframe.

```{r}
# Define number of principal components to keep
num_pc <- 5  # Example value, replace with actual num_pc

# Subset loadings (select first `num_pc` principal components)
loading_df <- pca$rotation[, 1:num_pc]

# Rename columns to "PC1", "PC2", ..., "PC{num_pc}"
colnames(loading_df) <- paste0("PC", seq(1, num_pc))

# Print first few rows
head(loading_df)

```

Again, the loadings are the *weights* with which the variables are combined to form the principal components. For example, the `PC1` column tells us that this component is equal to:

$$(-0.020055\times\text{women}) + (0.289614\times\text{white}) + (0.050698\times\text{citizen}) + \dots$$

Since the components together capture over half the total variation, the heavily weighted variables in the selected components are the ones that drive variation in the original data.

By visualizing the loadings, we can see which variables are most influential for each component, and thereby also which variables seem to drive total variation in the data.

```{r}
# Load necessary libraries
library(tidyr)
library(dplyr)
library(ggplot2)

# Melt from wide to long format
loading_plot_df <- loading_df %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Variable") %>%
  pivot_longer(cols = -Variable, names_to = "Principal_Component", values_to = "Loading")

# Add a column of zeros for x = 0 reference line
loading_plot_df <- loading_plot_df %>%
  mutate(zero = 0)

# Create base plot
loadings_plot <- ggplot(loading_plot_df, aes(x = Loading, y = Variable, color = Principal_Component)) +
  
  # Add lines + points for loadings
  geom_line(aes(group = Variable)) +
  geom_point() +
  
  # Add vertical reference line at x = 0
  geom_vline(aes(xintercept = zero), color = "black", linetype = "dashed", size = 0.5) +
  
  # Facet by Principal Component
  facet_wrap(~ Principal_Component, scales = "free_y") +
  
  # Adjust labels and theme
  labs(x = "Loading", y = "", color = "Principal Component") +
  theme_minimal() +
  theme(legend.position = "none") # Hide legend since facets already distinguish components

# Display the plot
print(loadings_plot)

```

Look first at PC1: the variables with the largest loadings (points farthest in either direction from the zero line) are Child Poverty (positive), Employed (negative), Income per capita (negative), Poverty (positive), and Unemployment (positive). We know from exploring the correlation matrix that employment rate, unemployment rate, and income per capita are all related, and similarly child poverty rate and poverty rate are related. Therefore, the positively-loaded variables are all measuring more or less the same thing, and likewise for the negatively-loaded variables.

Essentially, then, PC1 is predominantly (but not entirely) a representation of income and poverty. In particular, counties have a higher value for PC1 if they have lower-than-average income per capita and higher-than-average poverty rates, and a smaller value for PC1 if they have higher-than-average income per capita and lower-than-average poverty rates.

#### A system for loading interpretation

Often interpreting principal components can be difficult, and sometimes there's no clear interpretation available! That said, it helps to have a system instead of staring at the plot and scratching our heads. Here is a semi-systematic approach to interpreting loadings:

1.  Divert your attention away from the zero line.
2.  Find the largest positive loading, and list all variables with similar loadings.
3.  Find the largest negative loading, and list all variables with similar loadings.
4.  The principal component represents the difference between the average of the first set and the average of the second set.
5.  Try to come up with a description of less than 4 words.

This system is based on the following ideas:

-   a high loading value (negative or positive) indicates that a variable strongly influences the principal component;

-   a negative loading value indicates that

    -   increases in the value of a variable *decrease* the value of the principal component

    -   and decreases in the value of a variable *increase* the value of the principal component;

-   a positive loading value indicates that

    -   increases in the value of a variable *increase* the value of the principal component

    -   and decreases in the value of a variable *decrease* the value of the principal component;

-   similar loadings between two or more variables indicate that the principal component reflects their *average*;

-   divergent loadings between two sets of variables indicates that the principal component reflects their *difference*.

## Question 6

Work with your neighbor to interpret PC2. Come up with a name for the component and explain which variables are most influential.

`r bfcolor("YOUR ANSWER:", "red")`

*(Type your answer here, replacing this text.)*

#### Standardization

Data are typically standardized because otherwise the variables on the largest scales tend to dominate the principal components, and most of the time PC1 will capture the majority of the variation. However, that is artificial. In the census data, income per capita has the largest magnitudes, and thus, the highest variance.

```{r}
# Compute column-wise variances
var_values <- apply(x_mx, 2, var)

# Sort in descending order and get top 3 variances
top_3_vars <- sort(var_values, decreasing = TRUE)[1:3]

# Print results
top_3_vars
```

When PCs are computed without normalization, the total variation is mostly just the variance of income per capita because it is orders of magnitude larger than the variance of any other variable. But that's just because of the *scale* of the variable -- incomes per capita are large numbers -- not a reflection that it varies more or less than the other variables.

Run the cell below to see what happens to the variance ratios if the data are not normalized.

```{r}
# Recompute PCA without standardization
pca_unscaled <- prcomp(x_mx, center = TRUE, scale. = FALSE)

# Compute variance ratios for the first three principal components
var_ratios_unscaled <- (pca_unscaled$sdev^2) / sum(pca_unscaled$sdev^2)

# Show variance ratios for the first three PCs
var_ratios_unscaled[1:3]

```

Further, let's look at the loadings when data are not standardized:

```{r}
# Subset loadings (first two principal components)
unscaled_loading_df <- pca_unscaled$rotation[, 1:2]

# Rename columns to "PC1", "PC2"
colnames(unscaled_loading_df) <- c("PC1", "PC2")

# Melt from wide to long format
unscaled_loading_plot_df <- unscaled_loading_df %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Variable") %>%
  pivot_longer(cols = -Variable, names_to = "Principal_Component", values_to = "Loading") %>%
  mutate(zero = 0)  # Add column for x = 0 reference line

# Create base plot
unscaled_loading_plot <- ggplot(unscaled_loading_plot_df, aes(x = Loading, y = Variable, color = Principal_Component)) +
  
  # Add lines + points for loadings
  geom_line(aes(group = Variable)) +
  geom_point() +
  
  # Add vertical reference line at x = 0
  geom_vline(aes(xintercept = zero), color = "black", linetype = "dashed", size = 0.5) +
  
  # Facet by Principal Component
  facet_wrap(~ Principal_Component, scales = "free_y") +
  
  # Adjust labels, theme, and title
  labs(x = "Loading", y = "", color = "Principal Component", title = "Loadings from Unscaled PCA") +
  theme_minimal() +
  theme(legend.position = "none")  # Hide legend since facets already distinguish components

# Display the plot
print(unscaled_loading_plot)

```

Notice that the variables with nonzero loadings in unscaled PCA are simply the three variables with the largest variances.

```{r}
# Compute column-wise variances
var_values <- apply(x_mx, 2, var)

# Sort in descending order and get top 3 variances
top_3_vars <- sort(var_values, decreasing = TRUE)[1:3]

# Print results
top_3_vars
```

### Exploratory analysis based on PCA

Now that we have the principal components, we can use them for exploratory data visualizations. To this end, let's retrieve the scores from the components you selected:

```{r}
# Subset scores (first `num_pc` principal components)
score_df <- as.data.frame(pca$x[, 1:num_pc])

# Rename columns to "PC1", "PC2", ..., "PC{num_pc}"
colnames(score_df) <- paste0("PC", seq(1, num_pc))

# Add State and County columns from census data
score_df <- cbind(score_df, census[, c("State", "County")])

# Print first few rows
head(score_df)
```

The PC's can be used to construct scatterplots of the data and search for patterns. We'll illustrate that by identifying some outliers. The cell below plots PC2 (employment type) against PC4 (carpooling?):

```{r}
# Create scatter plot of PC2 vs. PC4
scatter_plot <- ggplot(score_df, aes(x = PC2, y = PC4)) +
  
  # Add scatter points with transparency (opacity = 0.2)
  geom_point(alpha = 0.2) +
  
  # Set axis labels
  labs(x = "Self-employment PC", y = "Carpooling PC") +
  
  # Use a minimal theme
  theme_minimal()

# Display the plot
print(scatter_plot)

```

Notice that there are a handful of outlying points in the upper right region away from the dense scatter. What are those?

In order to inspect the outlying counties, we first need to figure out how to identify them. The outlying values have a large *sum* of PC2 and PC4. We can distinguish them by finding a cutoff value for the sum; a simple quantile will do.

```{r}
# Compute cutoff value (99.999th percentile) for PC2 + PC4
pc2_pc4_sum <- score_df$PC2 + score_df$PC4
cutoff <- quantile(pc2_pc4_sum, probs = 0.99999)

# Store outlying rows using cutoff
outliers <- score_df %>% filter((-PC2 + PC4) > cutoff)

# Create scatter plot of all data points
scatter_plot <- ggplot(score_df, aes(x = PC2, y = PC4)) +
  
  # Base scatter points (opacity = 0.2)
  geom_point(alpha = 0.2, color = "black") +
  
  # Overlay outliers in red (opacity = 0.3)
  geom_point(data = outliers, aes(x = PC2, y = PC4), color = "red", alpha = 0.3) +
  
  # Set axis labels
  labs(x = "Self-employment PC", y = "Carpooling PC", title = "Scatter Plot with Outliers") +
  
  # Use a minimal theme
  theme_minimal()

# Display the plot
print(scatter_plot)

```

Notice that almost all the outlying counties are remote regions of Alaska:

```{r}
# Print the outliers
print(outliers)
```

What sets them apart? The cell below retrieves the normalized data and county name for the outlying rows, and then plots the Standardized values of each variable for all 9 counties as vertical ticks, along with a point indicating the mean for the outlying counties. This plot can be used to determine which variables are over- or under-average for the outlying counties relative to the nation by simply locating means that are far from zero in either direction.

```{r}
# Standardize x_mx (mean = 0, std = 1)
x_ctr <- as.data.frame(scale(x_mx))  # Center and scale data

# Convert row names of outliers to numeric indices
outlier_indices <- match(outliers$County, census$County)

# Retrieve normalized data for outlying rows & join with County info
outlier_data <- x_ctr %>%
  slice(outlier_indices) %>%  # Select only outlier rows
  mutate(County = census$County[outlier_indices])  # Add County column

# Melt (reshape) data from wide to long format
outlier_plot_df <- outlier_data %>%
  pivot_longer(cols = -County, names_to = "Variable", 
               values_to = "Standardized_value")

# Compute means of each variable across counties
means_df <- outlier_plot_df %>%
  group_by(Variable) %>%
  summarize(group_mean = mean(Standardized_value)) %>%
  mutate(large = abs(group_mean) > 3)  # Flag large deviations

# Create base plot
ticks_plot <- ggplot(outlier_plot_df, 
                     aes(x = Standardized_value, y = Variable)) +
  
  # Add tick marks for outlier values
  geom_point(shape = "|", size = 3) +
  
  # Add vertical lines for ±3 SD range
  geom_vline(xintercept = c(-3, 3), linetype = "dashed", color = "gray") +
  
  # Add mean markers
  geom_point(data = means_df, 
             aes(x = group_mean, y = Variable, color = large), 
             size = 3) +
  
  # Axis labels and minimal theme
  labs(x = "Standardized Value", y = "Variable", 
       title = "Outlier Standardized Values") +
  theme_minimal() +
  theme(legend.position = "none")

# Display plot
print(ticks_plot)
```

## Question 7

The two variables that clearly set the outlying counties apart from the nation are the percentage of the population using alternative transportation (extremely above average) and the percentage that drive to work (extremely below average). What about those counties explains this?

(*Hint:* take a peek at the [Wikipedia page on transportation in Alaska](https://en.wikipedia.org/wiki/Transportation_in_Alaska).)

`r bfcolor("YOUR ANSWER:", "red")`

*(Type your answer here, replacing this text.)*

## Submission

1.  Save the notebook.\
2.  Restart the kernel and run all cells. (**CAUTION**: if your notebook is not saved, you will lose your work.)\
3.  Carefully look through your notebook and verify that all computations execute correctly. You should see **no errors**; if there are any errors, make sure to correct them before you submit the notebook.\
4.  Download the notebook as an `.qmd` file. This is your backup copy.\
5.  Export the notebook as PDF and upload to Canvas.
